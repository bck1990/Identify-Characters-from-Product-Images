{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to install the latest version of PyTorch and PIL for this.\n",
    "\n",
    "Get from here: https://pytorch.org/get-started/locally/ and https://pypi.org/project/Pillow/2.2.1/\n",
    "\n",
    "Below code is for windows only.\n",
    "\n",
    "## GPU memory of at least 8 GB is needed to run this or the error 'CUDA out of memory' will show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgOgwYWGnPvg"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPo780e8nPvj"
   },
   "source": [
    "## Training Phase\n",
    "## Load the training and validation data\n",
    "\n",
    "### NOTE: Right now the validation directory has only 1 dummy image (refer to last part of the Report pdf document for reason). \n",
    "\n",
    "From report:\n",
    "\"Before the final submission, only 1 dummy image is placed in the validation directory (as this code requires validation directory with at least 1 image to run, feel free to randomly copy and paste any amount of images from the training set to this directory to use as validation images when running it). This is to ensure that the maximum number of images are used for training instead of validation as previous validation runs have already identified the best model.\"\n",
    "\n",
    "Feel free to randomly copy and paste any amount of images from the training set to this directory to use as validation images when running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNvc8C_5nPvl"
   },
   "outputs": [],
   "source": [
    "#train_dir = 'train'\n",
    "train_dir = 'train_expanded_character'\n",
    "test_dir = 'test'\n",
    "valid_dir = 'valid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "V91jgHtoBde5",
    "outputId": "fa59445b-1a3a-47bf-b3c4-b092a08ced48"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor for validation set\n",
    "# validation set is preprocessed to be of appropriate shape to fit into the ResNext model\n",
    "# validation set's preprocessing does not include transfomration for augmentation\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                      [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# training set is preprocessed like validation \n",
    "# but has the additional transformation for random brightness, for augmentation\n",
    "augmentation_transforms = transforms.Compose([transforms.Resize(255),                                      \n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ColorJitter(brightness=.05, saturation=0, contrast=0),  #for AUGMENTATION                        \n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                      [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# load the training and test datasets with the appropriate preprocessing\n",
    "train_data = datasets.ImageFolder(train_dir, transform=augmentation_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "\n",
    "# obtain indices for training and validation\n",
    "num_train = len(train_data)\n",
    "print(\"Total number of training images: \" + str(num_train))\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(np.floor(num_train))\n",
    "train_idx = indices[:train_size]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(list(range(len(valid_data))))\n",
    "\n",
    "# prepare data loaders to be used in training model later (prepare data in batches)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2kL_tO3nPvt"
   },
   "source": [
    "# Building and training the classifier\n",
    "\n",
    "\n",
    "* Load a [pre-trained network](http://pytorch.org/docs/master/torchvision/models.html)\n",
    "* Freeze some top layers while only training the bottom layers' weights\n",
    "* Define a fully-connected layer below\n",
    "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
    "* Track the loss and accuracy on the validation set to determine the best hyperparameters\n",
    "\n",
    "### Note that validation directory has only 1 dummy image as mentioned earlier at the start of this notebook, under 'load the training and validation data' section. To properly look at validation accuracy, feel free to randomly copy and paste any amount of images from the training set to this directory to use as validation images when running it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11054
    },
    "colab_type": "code",
    "id": "7s35uLjPnPvu",
    "outputId": "f7e1c5f8-71ce-4c4d-b222-72140676b4e7"
   },
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "model = models.resnext101_32x8d(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11138
    },
    "colab_type": "code",
    "id": "rf7Ay6u7nPvz",
    "outputId": "2f62457e-eefa-4eec-d686-ed4ade596a80"
   },
   "outputs": [],
   "source": [
    "layer_freeze_onwards = 18\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.bn1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.relu.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.maxpool.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layer1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layer2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# in layer 3, freeze all weights until the 18th sub-layer, refer to Appendix in the code for more details\n",
    "# layers above this are all frozen and not trained\n",
    "for param in model.layer3[0:layer_freeze_onwards].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace FC layers of pretrained model\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 42))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displays the layers in layer 3 that are going to be trained\n",
    "### Note that all layers in layer 4 are going to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1579
    },
    "colab_type": "code",
    "id": "wOyFEWbUwIjt",
    "outputId": "2fc26cfd-bb5f-473c-fa49-ba0460d819f9"
   },
   "outputs": [],
   "source": [
    "model.layer3[layer_freeze_onwards:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6omCkbjUnPv3",
    "outputId": "3e4debb0-6d1e-4a67-f18e-25b840076816"
   },
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Optimizers and the Respective Learning Rate, and Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sx-Yz4MtnPv6"
   },
   "outputs": [],
   "source": [
    "# No need for logsoftmax on model output when this is used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer1 = optim.Adam(model.layer3[layer_freeze_onwards:].parameters(), lr=0.00005)\n",
    "optimizer2 = optim.Adam(model.layer4.parameters(), lr=0.00005)\n",
    "optimizer3 = optim.Adam(model.fc.parameters(), lr=0.00005)\n",
    "\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of model weights, forward and back propagation, calulating train/validation loss and accuracy\n",
    "### Do NOT worry about validation accuracy as there is only 1 image in validation folder as mentioned earlier, refer to last section of report for more details.\n",
    "### First run 4 epochs at learning rate 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "P21tbBCznPv9",
    "outputId": "bb31cd22-8894-4155-f894-c8c205ab42e3"
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 4\n",
    "\n",
    "# track change in validation loss, start with maximum loss, i.e. infinity\n",
    "valid_loss_min = np.Inf \n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    valid_accuracy = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        correct_tensor = top_class.eq(target.data.view_as(top_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        train_accuracy += np.mean(correct)\n",
    "     \n",
    "          \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(validloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        correct_tensor = top_class.eq(target.data.view_as(top_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        valid_accuracy += np.mean(correct)\n",
    "        \n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(validloader.dataset)\n",
    "    train_accuracy = train_accuracy/len(trainloader)\n",
    "    valid_accuracy = valid_accuracy/len(validloader)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t Training Acc: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f}'.format(\n",
    "        epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
    "    \n",
    "    # check if validation loss has decreased\n",
    "    # this requires \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change learning rate to 0.00001, run 4 more epochs, the full code for setting optimizer and training model is repeated in the next 2 blocks for ease of customization should the need arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for logsoftmax on model output when this is used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer1 = optim.Adam(model.layer3[layer_freeze_onwards:].parameters(), lr=0.00001)\n",
    "optimizer2 = optim.Adam(model.layer4.parameters(), lr=0.00001)\n",
    "optimizer3 = optim.Adam(model.fc.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 4\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    valid_accuracy = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        correct_tensor = top_class.eq(target.data.view_as(top_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        train_accuracy += np.mean(correct)\n",
    "     \n",
    "          \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(validloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        correct_tensor = top_class.eq(target.data.view_as(top_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        valid_accuracy += np.mean(correct)\n",
    "        \n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(validloader.dataset)\n",
    "    train_accuracy = train_accuracy/len(trainloader)\n",
    "    valid_accuracy = valid_accuracy/len(validloader)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t Training Acc: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f}'.format(\n",
    "        epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
    "    \n",
    "    # check if validation loss has decreased\n",
    "    # this requires \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change learning rate to 0.000001, run 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for logsoftmax on model output when this is used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer1 = optim.Adam(model.layer3[layer_freeze_onwards:].parameters(), lr=0.000001)\n",
    "optimizer2 = optim.Adam(model.layer4.parameters(), lr=0.000001)\n",
    "optimizer3 = optim.Adam(model.fc.parameters(), lr=0.000001)\n",
    "\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    valid_accuracy = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        correct_tensor = top_class.eq(target.data.view_as(top_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        train_accuracy += np.mean(correct)\n",
    "     \n",
    "          \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(validloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        correct_tensor = top_class.eq(target.data.view_as(top_class))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        valid_accuracy += np.mean(correct)\n",
    "        \n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(validloader.dataset)\n",
    "    train_accuracy = train_accuracy/len(trainloader)\n",
    "    valid_accuracy = valid_accuracy/len(validloader)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t Training Acc: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f}'.format(\n",
    "        epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
    "    \n",
    "    # check if validation loss has decreased\n",
    "    # this requires \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YI1ixOkfnPwL"
   },
   "source": [
    "## Prediction Phase\n",
    "### Load the test data\n",
    "\n",
    "Similar to loading training and validation data, same preprocessing steps, however code is copied here to allow for any adjustments if needed (not encouraged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFaWqux6OA1W"
   },
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# specify test directory\n",
    "test_dir = 'test'\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                      [0.229, 0.224, 0.225])])\n",
    "\n",
    "# choose the training and test datasets\n",
    "test_data = ImageFolderWithPaths(test_dir,transform=test_transforms)\n",
    "\n",
    "\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model in evaluation mode through data, predict the top 5 classes for data using the top 5 probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1179
    },
    "colab_type": "code",
    "id": "C4dfMKVFbatY",
    "outputId": "9d0aa8fe-505f-40d3-af1e-9bd76a2b5235"
   },
   "outputs": [],
   "source": [
    "    idx_to_class = {value: key for key, value in train_data.class_to_idx.items()}\n",
    "    idx_to_class\n",
    "    \n",
    "    predicted = []\n",
    "    data_to_predict =[]\n",
    "  \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target, paths) in enumerate(testloader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        top_p, top_class = output.topk(5, dim=1)\n",
    "        top_class_np = top_class.cpu().numpy()\n",
    "        for each in top_class_np:\n",
    "          for idx in each:\n",
    "            predicted.append(idx_to_class[idx])\n",
    "            \n",
    "        for path in paths:\n",
    "              data_to_predict.append(path)\n",
    "        print('Done with batch ',batch_idx)\n",
    "        \n",
    "    predicted_array = np.array(predicted)\n",
    "    print(predicted_array.reshape(-1,5))  \n",
    "    print(data_to_predict)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only the top class is needed as we just want the class with the highest probability, extract this top class and print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "klGMGLQkjTtN",
    "outputId": "715c6d34-ef49-48ea-daee-0252777a4a35"
   },
   "outputs": [],
   "source": [
    "predicted_array_reshaped = predicted_array.reshape(-1,5)\n",
    "\n",
    "predicted_list = []\n",
    "dataname_list = []\n",
    "\n",
    "for i, name in enumerate(predicted_array_reshaped):\n",
    "  string = predicted_array_reshaped[i][0] #only extract the highest probability class\n",
    "  predicted_list.append(string)\n",
    "  \n",
    "print(predicted_list)\n",
    "\n",
    "for name in data_to_predict:\n",
    "  dataname_list.append(name[11:])\n",
    "  \n",
    "print(dataname_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe of prediction and output to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R9TvrON7lpHQ",
    "outputId": "b00e58d5-37fe-4baa-f8aa-9841447172f2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Filename': dataname_list, 'Prediction': predicted_list})\n",
    "df['Prediction'].nunique()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "teWXMoFM2W3I"
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPJA2AtsfXBJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "submit__submit_0.1augmented corr_cax_characters_resnext101_32x8d _18_layer_onwards_in_3_unfreezed",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
